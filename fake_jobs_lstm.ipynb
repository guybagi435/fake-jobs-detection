{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "fake-jobs-lstm.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "OClWhZQpM9Sq"
      },
      "source": [
        "\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "kIeW-Db2M9Sv"
      },
      "source": [
        "df=pd.read_csv(\"/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UeO_tsKmM9Sy"
      },
      "source": [
        "df[\"relevant_data\"]=df[\"company_profile\"]+df[\"description\"]+df[\"requirements\"]\n",
        "df=df[[\"relevant_data\",\"fraudulent\"]]\n",
        "df.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "XJ-PE7-tM9S1"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df.iloc[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "C5BgJiJsM9S3"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "x_data=[]\n",
        "for i in range(12631):\n",
        "    x_data.append([word.lower() for word in word_tokenize(df.iloc[i,0]) if not word in stopwords.words('english') and not 'http' in word])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Dfh1K7kLM9S6"
      },
      "source": [
        "for i in x_data:\n",
        "    for x in i:\n",
        "        x=x[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "a65nO_EiM9S9"
      },
      "source": [
        "x_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jBRF1uBDM9TD"
      },
      "source": [
        "import gensim\n",
        "\n",
        "w2v_model = gensim.models.Word2Vec(sentences=x_data, size=100, window=2, min_count=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ETHju91WM9TH"
      },
      "source": [
        "w2v_model.wv.most_similar(\"talented\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "as6dqcF0M9TJ"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_data)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(x_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bRR7XR82M9TL"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4gHy_EdXM9TN"
      },
      "source": [
        "X = pad_sequences(X, maxlen=700)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i5-jrNGUM9TP"
      },
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "def get_weight_matrix(model, vocab):\n",
        "    # total vocabulary size plus 0 for unknown words\n",
        "    vocab_size = len(vocab) + 1\n",
        "    # define weight matrix dimensions with all 0\n",
        "    weight_matrix = np.zeros((vocab_size, 100))\n",
        "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
        "    for word, i in vocab.items():\n",
        "        weight_matrix[i] = model[word]\n",
        "    return weight_matrix\n",
        "\n",
        "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xcXOUuwQM9TR"
      },
      "source": [
        "w2v_model[\"we\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "MneIDc3jM9TU"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "#Non-trainable embeddidng layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, output_dim=100, weights=[embedding_vectors], input_length=700, trainable=False))\n",
        "#LSTM \n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df[\"fraudulent\"].values) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uIye_rGgM9TW"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=12,validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}